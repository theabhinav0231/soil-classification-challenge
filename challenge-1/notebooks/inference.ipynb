{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QN9EsvyczM_",
        "outputId": "f1c2e249-e31b-4d90-c25b-26e7ac9af632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.6'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seed for reproduciblity\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "8vHtVOklc3-S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation\n",
        "def get_test_transforms(img_size=384):\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "24hTZLmfc9QK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input test data structure\n",
        "class SoilTestDataset(Dataset):\n",
        "    \"\"\"Dataset class for test data\"\"\"\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image_id'])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img)\n",
        "            img = augmented['image']\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "Go1Ih9MvdUJR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing model\n",
        "class SoilClassifier(nn.Module):\n",
        "    def __init__(self, model_name='efficientnet_b3', pretrained=False, num_classes=4):\n",
        "        super(SoilClassifier, self).__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
        "\n",
        "        # Get the number of features in the last layer\n",
        "        if 'efficientnet' in model_name:\n",
        "            n_features = self.model.classifier.in_features\n",
        "            self.model.classifier = nn.Identity()\n",
        "        else:  # For other models like ResNet\n",
        "            n_features = self.model.fc.in_features\n",
        "            self.model.fc = nn.Identity()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(n_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        features = self.dropout(features)\n",
        "        return self.classifier(features)"
      ],
      "metadata": {
        "id": "3xwOb_lSdWKP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, images in enumerate(tqdm(dataloader, desc=\"Predicting\")):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store results\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "    return predictions, probabilities"
      ],
      "metadata": {
        "id": "eEcaYd6pdpU-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    set_seed(42)\n",
        "\n",
        "    # Device agnostic code\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # File paths\n",
        "    TEST_DIR = '/kaggle/input/soil-classification/soil_classification-2025/test'\n",
        "    TEST_CSV = '/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv'\n",
        "    MODEL_PATH = '/kaggle/input/your-model-path/model.pth'\n",
        "\n",
        "    # Configuration\n",
        "    CONFIG = {\n",
        "        'IMG_SIZE': 384,\n",
        "        'BATCH_SIZE': 32,  # Increased batch size for faster inference\n",
        "        'NUM_WORKERS': 4,\n",
        "        'MODEL_NAME': 'efficientnet_b3',\n",
        "        'NUM_CLASSES': 4,\n",
        "        'DEVICE': device,\n",
        "    }\n",
        "\n",
        "    # Soil type mapping\n",
        "    soil_types = {\n",
        "        'Alluvial soil': 0,\n",
        "        'Black Soil': 1,\n",
        "        'Clay soil': 2,\n",
        "        'Red soil': 3\n",
        "    }\n",
        "\n",
        "    # Reverse mapping for predictions\n",
        "    idx_to_soil = {v: k for k, v in soil_types.items()}\n",
        "    print(\"Soil type mapping:\")\n",
        "    for k, v in soil_types.items():\n",
        "        print(f\"{v}: {k}\")\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.read_csv(TEST_CSV)\n",
        "    print(f\"\\nTest data shape: {test_df.shape}\")\n",
        "    print(\"Sample of test data:\")\n",
        "    print(test_df.head())\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    print(\"\\nLoading pre-trained model...\")\n",
        "    model = SoilClassifier(\n",
        "        model_name=CONFIG['MODEL_NAME'],\n",
        "        pretrained=False,\n",
        "        num_classes=CONFIG['NUM_CLASSES']\n",
        "    ).to(CONFIG['DEVICE'])\n",
        "\n",
        "    # Load the trained weights\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=CONFIG['DEVICE']))\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Create test dataset and dataloader\n",
        "    test_dataset = SoilTestDataset(\n",
        "        test_df,\n",
        "        TEST_DIR,\n",
        "        transform=get_test_transforms(CONFIG['IMG_SIZE'])\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        shuffle=False,\n",
        "        num_workers=CONFIG['NUM_WORKERS'],\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTest dataset size: {len(test_dataset)}\")\n",
        "    print(f\"Number of batches: {len(test_loader)}\")\n",
        "\n",
        "    # Make predictions\n",
        "    predictions, probabilities = make_predictions(model, test_loader, CONFIG['DEVICE'])\n",
        "\n",
        "    print(f\"\\nGenerated {len(predictions)} predictions\")\n",
        "    print(\"Prediction distribution:\")\n",
        "    pred_counts = np.bincount(predictions)\n",
        "    for i, count in enumerate(pred_counts):\n",
        "        if count > 0:\n",
        "            print(f\"{idx_to_soil[i]}: {count} ({count/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "    # Convert predictions to soil type names\n",
        "    predicted_soil_types = [idx_to_soil[pred] for pred in predictions]\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission_df = test_df.copy()\n",
        "    submission_df['soil_type'] = predicted_soil_types\n",
        "\n",
        "    print(\"\\nSubmission dataframe:\")\n",
        "    print(submission_df.head())\n",
        "    print(f\"Submission shape: {submission_df.shape}\")\n",
        "\n",
        "    # Verify submission format\n",
        "    print(\"\\nVerifying submission format...\")\n",
        "    print(f\"Required columns: ['image_id', 'soil_type']\")\n",
        "    print(f\"Actual columns: {list(submission_df.columns)}\")\n",
        "    print(\"\\nUnique soil types in submission:\")\n",
        "    print(submission_df['soil_type'].value_counts())\n",
        "\n",
        "    # Check for any missing values\n",
        "    if submission_df.isnull().sum().sum() > 0:\n",
        "        print(\"\\nWarning: Found missing values in submission!\")\n",
        "        print(submission_df.isnull().sum())\n",
        "    else:\n",
        "        print(\"\\nNo missing values found. Submission looks good!\")\n",
        "\n",
        "    # Save submission file\n",
        "    submission_file = 'submission.csv'\n",
        "    submission_df[['image_id', 'soil_type']].to_csv(submission_file, index=False)\n",
        "\n",
        "    print(f\"\\nSubmission file '{submission_file}' created successfully!\")\n",
        "    print(f\"File size: {os.path.getsize(submission_file)} bytes\")\n",
        "\n",
        "    # Display first few rows of the final submission\n",
        "    print(\"\\nFinal submission (first 10 rows):\")\n",
        "    print(pd.read_csv(submission_file).head(10))\n",
        "\n",
        "    # Display prediction confidence statistics\n",
        "    print(\"\\nPrediction Confidence Statistics:\")\n",
        "    probabilities_array = np.array(probabilities)\n",
        "    max_probs = np.max(probabilities_array, axis=1)\n",
        "\n",
        "    print(f\"Mean confidence: {np.mean(max_probs):.4f}\")\n",
        "    print(f\"Median confidence: {np.median(max_probs):.4f}\")\n",
        "    print(f\"Min confidence: {np.min(max_probs):.4f}\")\n",
        "    print(f\"Max confidence: {np.max(max_probs):.4f}\")\n",
        "\n",
        "    # Count predictions by confidence level\n",
        "    high_conf = np.sum(max_probs > 0.9)\n",
        "    med_conf = np.sum((max_probs > 0.7) & (max_probs <= 0.9))\n",
        "    low_conf = np.sum(max_probs <= 0.7)\n",
        "\n",
        "    print(f\"\\nConfidence distribution:\")\n",
        "    print(f\"High confidence (>0.9): {high_conf} ({high_conf/len(max_probs)*100:.1f}%)\")\n",
        "    print(f\"Medium confidence (0.7-0.9): {med_conf} ({med_conf/len(max_probs)*100:.1f}%)\")\n",
        "    print(f\"Low confidence (<=0.7): {low_conf} ({low_conf/len(max_probs)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"INFERENCE COMPLETE!\")\n",
        "    print(\"submission.csv file created.\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "AjYEU8ckdrrH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "deCGZO0oeBjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BdW1a0TgeDsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}